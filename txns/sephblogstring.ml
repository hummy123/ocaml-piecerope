let str = "> DRAFT: Needs diagrams, maybe some tidy up before publishing. Don't share the link too widely please! This isn't ready for HN.\n\n# CRDTs go BRRR: Making collaborative editing 4000x faster\n\nA few years ago I was really bothered by an academic paper.\n\nSome researchers in France put together a comparison showing lots of ways you could implement concurrent editing, using various CRDT and OT algorithms. And they benchmarked all of them. (Wow, yess!) Some algorithms worked reasonably well. But others took upwards of 3 seconds to process simple paste operations from their editing sessions. Yikes!\n\nWhich algorithm was that? Well, this is awkward but .. it was mine. I mean, I didn't invent it - but it was the algorithm I was using for ShareJS. The algorithm we used for Google Wave. The algorithm which - hang on - I knew for a fact didn't take 3 seconds to process large paste events. Whats going on here?\n\nI took a closer look at the paper. In their implementation when a user pasted a big chunk of text (say, 1000 characters), instead of creating 1 operation with 1000 characters, their code split the insert into 1000 individual operations. And each of those operations needed to be processed separately. Do'h - of course it'll be slow if you do that! This isn't a problem with the algorithm. This is just a problem with *your particular implementation*.\n\nThe annoying part was that several people sent me links to the paper and (pointedly) asked me what I think about it. Written up as a Published Science Paper, these speed comparisons seemed like a Fact About The Universe. And not what they really were - implementation details of some java code, written by a probably overstretched researcher. One of a whole bunch of implementations they needed to code up.\n\n\"Nooo! The peer reviewed science isn't right everybody! Please believe me!\". But I didn't have a published paper justifying my claims. I had working code but it felt like none of the smart computer science people cared about that. Who was I? I was nobody.\n\n---\n\nWhen we think about CRDTs and other collaborative editing systems we have a language problem. We describe each system as an \"algorithm\". Jupiter is an Algorithm. RGA is an Algorithm. But really there's two very separate aspects:\n\n1. The *semantics* of the concurrent editing system. When concurrent edits happen in the same location in the document, what happens? What are the rules which describe the system's behaviour?\n2. The *implementation*. What language are we using? What data structures? How well optimized is it, and for what scenarios?\n\nIf my implementation runs slowly, what does that actually tell us? Maybe its like tests. A passing test suite suggests, but can never prove that there are no bugs. And a slow implementation suggests, but can never prove that this system will always be slow. If you wait long enough, there's always more bugs. And, maybe, writes faster implementations.\n\nI've translated my old text OT code into C, Javascript, Go, Rust and Swift. Each implementation has the same semantics, and the same algorithm. But the performance is not even close. In javascript my transform function ran about 100 000 times per second. Not bad! But the same function in C does 20M iterations per second. That's 200x faster. Wow!\n\nWere the academics testing the slow version or the fast version of this code? Maybe, without noticing, they had fast versions of some algorithms and slow versions of others. Its impossible to tell from the paper!\n\n\n# Making CRDTs fast\n\nSo as you may know, I've been getting interested in CRDTs lately. I think they're the [future of collaborative editing](https://josephg.com/blog/crdts-are-the-future/). And maybe the future of all software - but I'm not ready to talk about that yet. Most CRDTs you read about in academic papers are crazy slow, and a decade ago I decided to stop reading academic papers and dismissed them. I assumed CRDTs had some inherent problem. A GUID for every character? Nought but madness comes from those strange lands! But - and this is awkward to admit - I think I've been making the same mistake as those researchers. I was reading papers which described the *semantics* of different systems. And I assumed that meant we knew how the best way to *implement* those systems. And wow, I was super wrong.\n\nHow wrong? Well. Running [this editing trace](https://github.com/automerge/automerge-perf/), [Automerge](https://github.com/automerge/automerge/) (a popular CRDT, written by [a popular researcher](https://martin.kleppmann.com/)) takes nearly 5 minutes to run. I have a new implementation that can process the same editing trace in 65 milliseconds. 0.065 seconds. Its nearly 5000x faster. Its the largest speed up I've ever gotten from optimization work - and I'm utterly delighted by it.\n\nLets talk about why automerge is currently slow, and I'll take you through all the steps toward making it super fast.\n\nWait, no. First we need to start with:\n\n## What is automerge?\n\nAutomerge is a library to help you do collaborative editing. Its written by Martin Kleppmann, who's a little bit famous from his book and [excellent talks](https://martin.kleppmann.com/2020/07/06/crdt-hard-parts-hydra.html). Its based on an algorithm called RGA, which you can read about in an academic paper if you're into that sort of thing.\n\nMartin explains automerge far better than I will in this talk from 2020:\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/x7drE24geUw?start=1237\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\nAutomerge (and Yjs and other CRDTs) think of a shared document as a list of characters. Each character in the document gets a unique ID, and whenever you insert into the document, you name what you're inserting after.\n\nImagine I type \"abc\" into an empty document. Automerge creates 3 items:\n\n- Insert 'a' id `(seph, 0)` after `ROOT`\n  - Insert 'b' id `(seph, 1)` after `(seph, 0)`\n    - Insert 'c' id `(seph, 2)` after `(seph, 1)`\n\nWe can draw this as a tree!\n\n> automerge1.drawio.svg\n\nLets say Mike inserts an 'X' between *a* and *b*, so we get \"aXbc\". Then we have:\n\n- Insert 'a' id `(seph, 0)` after `ROOT`\n  - Insert 'X' id `(mike, 0)` after `(seph, 0)`\n  - Insert 'b' id `(seph, 1)` after `(seph, 0)`\n    - Insert 'c' id `(seph, 2)` after `(seph, 1)`\n\n> automerge2.drawio.svg\n\nNote the 'X' and 'b' both share the same parent. This will happen when users type concurrently in the same location in the document. But how do we figure out which character goes first? We could just sort using their agent IDs or something. But argh, if we do that the document could end up as *abcX*, even though Mike inserted *X* before the *b*. That would be really confusing.\n\nAutomerge (RGA) solves this with a neat hack. It adds an extra integer to each item called a *sequence number*. Whenever you insert something, you set the new item's sequence number to be 1 bigger than the biggest sequence number you've ever seen:\n\n- Insert 'a' id `(seph, 0)` after `ROOT`, seq: *0*\n  - Insert 'X' id `(mike, 0)` after `(seph, 0)`, seq: *3*\n  - Insert 'b' id `(seph, 1)` after `(seph, 0)`, seq: *1*\n    - Insert 'c' id `(seph, 2)` after `(seph, 1)`, seq: *2*\n\nIts the algorithmic version of \"Wow I saw a sequence number, and it was *this big!*.\" \"Yeah? Mine is *even bigger!*\".\n\nThe rule is that children are sorted first based on their sequence numbers (bigger sequence number first). If the sequence numbers match, the changes must be concurrent. In that case we can sort them arbitrarily based on their agent IDs. (We do it this way so all peers end up with the same resulting document.)\n\nYjs - which we'll see more of later - implements a CRDT called YATA. YATA is identical to RGA, except that it solves this problem with a slightly different hack. But the difference isn't really important here.\n\nAutomerge (RGA)'s *semantics* are:\n\n- Build the tree, connecting each item to its parent\n- When an item has multiple children, sort them by sequence number then by their ID.\n- The resulting list (or text document) can be made by flattening the tree with a depth-first traversal.\n\nSo how should you *implement* automerge? The automerge library does it in the obvious way, which is to store all the data as a tree. (At least I think so - after typing \"abc\" [this is automerge's internal state](https://gist.github.com/josephg/0522c4aec5021cc1dddb60e778828dbe). Uh, uhm, I have no idea whats going on here. And what are all those Uint8Arrays doing all over the place? Whatever. The automerge library works by building a tree of items.\n\nFor a simple benchmark, I'm going to test automerge using [an editing trace Martin himself made](https://github.com/automerge/automerge-perf/). This is a character by character recording of Martin typing up an academic paper. There aren't any concurrent edits in this trace, but users almost never actually put their cursors at exactly the same place and type anyway, so I'm not too worried about that. I'm also only counting the time taken to apply this trace *locally*, which isn't ideal but it'll do. Kevin Jahns (Yjs's author) has a much more [extensive benchmarking suite here](https://github.com/dmonad/crdt-benchmarks) if you're into that sort of thing. All the benchmarks here are done on my chonky ryzen 5800x workstation, with Nodejs 16.1 and rustc 1.52 when that becomes appropriate. (Spoilers!)\n\nThe editing trace has 260 000 edits, and the final document size is about 100 000 characters.\n\nAs I said above, automerge takes a little under 5 minutes to process this trace. And by the time its done, automerge uses 880 MB of RAM. Whoa! That's 10kb of ram *per key press*.\n\nTo get a sense of how much overhead there is, I'll compare this to a baseline where we just splice all the edits directly into a javascript string. This throws away the information we need to do collaborative editing, but it gives us a sense of how fast javascript can go. It turns out javascript running on V8 is *fast*:\n\n| Test                       | Time taken | RAM usage |\n|:-------------------------- | ----------:| ---------:|\n| **automerge (v1.0.0-preview2)** | 291s       | 880 MB    |\n| JS baseline                | 0.61s      | 0.1 MB    |\n\nThis is a chart of automerge\n\n\n\n## Why is automerge slow though?\n\nAutomerge is slow for a whole slew of reasons:\n\n1. Automerge's core tree based data structure gets big and slow as the document grows.\n2. Automerge makes heavy use of [Immutablejs](https://immutable-js.github.io/). Immutablejs is a library which gives you clojure-like copy-on-write semantics for javascript objects. This is a cool set of functionality, but the V8 optimizer & GC struggles to optimize code that uses immutablejs. As a result, it increases memory usage and decreases performance.\n3. Automerge treats each inserted character as a separate item. Remember that paper I talked about earlier, where copy+paste operations are slow? Automerge does that too!\n\nAutomerge was just never written with performance in mind. Their team is working on a replacement [rust implementation of the algorithm](https://github.com/automerge/automerge-rs/) to run through wasm, but at the time of writing it hasn't landed yet. I got the master branch working, but they obviously have some kinks to work out - on this test its barely faster than automerge's javascript code.\n\n---\n\nThere's an old saying with performance tuning:\n\n> You can't make the computer faster. You can only make it do less work.\n\nHow do we make the computer do less work here? There's lots of performance wins to be had from going through the code and improving lots of small things. But the automerge team has the right approach. Its always best to start with macro optimizations. Fix the core algorithm and data structures before moving to optimizing individual methods. There's no point optimizing a function when you're about to throw it away in a rewrite.\n\nBy far, Automerge's biggest problem is its complex tree based data structure. And we can replace it with something faster.\n\n\n## Improving the data structure\n\nLuckily, there's a better way to implement CRDTs, pioneered in Yjs. Yjs is another (competing) CRDT implementation on github made by Kevin Jahns. And its really fast, well documented and well made. If I were going to build software which supports collaborative editing today, I'd use Yjs.\n\nYjs doesn't need a whole blog post talking about how to make it fast because its already pretty fast, as we'll see soon. It got there by using a clever, obvious data structure \"trick\" that I don't think anyone else in the field has noticed. Instead of implementing the CRDT as a tree like automerge does:\n\n```javascript\nstate = {\n  { item: 'a', id: ['seph', 0], seq: 0, children: [\n    { item: 'X', id: [..], seq, children: []},\n    { item: 'b', id: [..], seq, children: [\n      { item: 'c', id: [..], seq, children: []}\n    ]}\n  ]}\n}\n```\n\nYjs just puts all the items in a single flat list:\n\n```javascript\nstate = [\n  { item: 'a', id: ['seph', 0], seq, parent: null },\n  { item: 'X', id, seq, parent: ['seph', 0] },\n  { item: 'b', id, seq, parent: ['seph', 0] },\n  { item: 'c', id, seq, parent: [..] }\n]\n```\n\nThat looks simple, but how do you insert a new item into a list? With automerge its easy:\n\n1. Find the parent item\n2. Insert the new item into the right location in the parents' list of children\n\nBut with this list approach its more complicated:\n\n1. Find the parent item\n2. Starting right after the parent item, iterate through the list until we find the location where the new item should be inserted (?)\n3. Insert it there, splicing into the array\n\nEssentially, this approach is just a fancy insertion sort. We're implementing a list CRDT with a list. Genius!\n\nIt sounds complicated - how do you figure out where the new item should go? But its complicated in the same way *math* is complicated. Its hard to understand, but once you understand it, you can implement the whole insert function in about 20 lines of code.\n\nI implemented both Yjs's CRDT (YATA) and Automerge myself using this approach. The [code is all here](https://github.com/josephg/reference-crdts/blob/main/crdts.ts) if you're curious and want more detail. [Here's the insert function in 50 lines, with comments.](https://github.com/josephg/reference-crdts/blob/fed747255df9d457e11f36575de555b39f07e909/crdts.ts#L401-L459). The yjs version of this function is in the same file, if you want to have a look. Despite being very different papers, the logic for inserting is almost identical.\n\nThis approach is better for lots of reasons:\n\n1. You can implement lots of CRDTs like this. Yjs, Automerge, Sync9 and others all work\n2. Doing it this way lets you implement the semantics of multiple CRDTs in the same codebase, if you want to.\n3. The algorithm does slow down when there are concurrent inserts in the same location. But that's super rare in practice - you almost always just insert right after the parent item. So in practice its really fast.\n4. Even though the implementation is different, this approach is *semantically* identical to the actual automerge, and yjs and sync9 code. ([Fuzzer verified (TM)](https://github.com/josephg/reference-crdts/blob/main/reference_test.ts))\n5. Its faster *and* simpler. Ideas which do this are rare and truly golden.\n\nIn my reference-crdts codebase I have an implementation of both RGA (automerge) and YATA (yjs). Their performance in this test is identical.\n\nBut my code, using this approach, ends up about 10x faster than automerge, and 30x more memory-efficient:\n\n| Test                              | Time taken | RAM usage |\n|:--------------------------        | ----------:| ---------:|\n| automerge (v1.0.0-preview2)       |  291s      | 880 MB    |\n| **reference-crdts (automerge / yjs)** |   31s      |  28 MB    |\n| JS baseline                       | 0.61s      | 0.1 MB    |\n\nI wish I could attribute *all* of that difference to this sweet and simple data structure. But a lot of the difference here is probably just immutablejs gumming things up.\n\n\n## Removing scanning\n\nWe're using a clean and fast core data abstraction now, but the implementation is still not *fast*. There are two big performance bottlenecks in this codebase we need to fix:\n\n1. Finding the location to insert, and\n2. Actually inserting into the array\n\nLets say we have a document, which is a list of items.\n\n```javascript\nstate = [\n  { item: 'a', isDeleted: false, id: ['seph', 0], seq, parent: null },\n  { item: 'X', isDeleted: false, id, seq, parent: ['seph', 0] },\n  { item: 'b', isDeleted: true,  id, seq, parent: ['seph', 0] },\n  { item: 'c', isDeleted: false, id, seq, parent: [..] },\n  ...\n]\n```\n\nSome of those items might have been deleted, so I've added an `isDeleted` flag to mark which ones. We can't just remove them from the array because other inserts might depend on them. (Drat! But that's a problem for another day.)\n\nImagine the document has 150 000 array items in it, representing 100 000 characters which haven't been deleted. If the user types an 'a' in the middle of the document (at *document position* 50 000), where is that in our array? To find out, we need to scan through the document (skipping deleted items) to figure out the right array location.\n\nSo if the user inserts at position 50 000, we'll probably have to scan past 75 000 items or something to find the insert position. Yikes!\n\nAnd then when we actually insert, the code does this, which is double yikes:\n\n```javascript\ndoc.content.splice(destIdx, 0, newItem)\n```\n\nIf the array currently has 150 000 items, javascript will need to move every single item *after* the new item once space forward in the array. This part happens in native code, but its still probably slow when we're moving so many items. (Aside: I'm assuming here that V8 uses an array internally. But who knows whats really going on down there.)\n\nInserting an item into a document with *n* items will take *n* steps. Wait, no - it's worse than that because deleted items stick around. Inserting into a document where there have *ever been* *n* items will take *n* steps. This algorithm isn't just slow. This system starts off fast, but it gets slower with every keystroke. Inserting *n* characters will take *O(n^2)*.\n\n> Can has, n^2 diagram?\n\n## Fixy fix fix\n\nCan we fix this? Yes we can! And by we, I mean yjs has fixed these problems. How did it do that?\n\nUsually when a human is typing, they don't actually move around the document very much. Yjs solves the scanning problem by caching the last *(index, position)* pair where the user edited the document. When they insert something later, they'll probably insert close to the old cursor position. So the system usually just scans forward or backwards from the cached location. This sounds a little bit dodgy (and it probably wouldn't work as well for non-text-editing applications). But it works great in practice.\n\nThen we need to insert efficiently, without copying all the existing items around. We can do that by replacing the array with a linked list. This is faster because linked lists allow you to insert in O(1) time.\n\nYjs does one extra optimization here: Because humans type in runs of characters, when we type \"hello\" in a document, instead of storing:\n\n```javascript\nstate = [\n  { item: 'h', isDeleted: false, id: ['seph', 0], seq, parent: null },\n  { item: 'e', isDeleted: false, id: ['seph', 1], seq, parent: ['seph', 0] },\n  { item: 'l', isDeleted: false, id: ['seph', 2], seq, parent: ['seph', 1] },\n  { item: 'l', isDeleted: false, id: ['seph', 3], seq, parent: ['seph', 2] },\n  { item: 'o', isDeleted: false, id: ['seph', 4], seq, parent: ['seph', 3] },\n]\n```\n\nYjs just stores:\n\n```javascript\nstate = [\n  { item: 'hello', isDeleted: false, id: ['seph', 0], seq, parent: null },\n]\n```\n\nAaah finally those pesky paste events will be fast!\n\nThis is just a compressed version of what we wrote above. The id, seq and parents of all the internal elements is implicit. (We assume the ids go up by 1 each time, and each item's parent is the previous item). So unfortunately we can't collapse the whole list into a single item or something like that. We can only collapse inserts when the IDs and parents line up sequentially. Yjs also needs to split items back out again if the user later inserts in the middle of one of these spans, so the logic is a bit complex.\n\nBut in this benchmarking data set, this change reduces the number of entries in our array from 180 000 down to 40 000.\n\nHow fast is it now? Yjs is 30x faster than the reference-crdts implementation, and it only uses about 10% as much RAM. Its *300x faster than automerge!*.\n\n| Test                              | Time taken | RAM usage |\n|:--------------------------        | ----------:| ---------:|\n| automerge (v1.0.0-preview2)       |  291s      | 880 MB    |\n| reference-crdts (automerge / yjs) |   31s      |  28 MB    |\n| **Yjs (v13.5.5)**                 | 0.97s      | 3.3 MB    |\n| JS baseline                       | 0.61s      | 0.1 MB    |\n\nHonestly I'm shocked and a little suspicious of how little ram yjs uses. I'm sure there's some wizardry in V8 which is making this possible, but its extremely impressive. Kevin says he wrote and rewrote parts of yjs 12 times in order to make this code run so fast. If there was a programmer version of the speedrunning community, they would adore Kevin.\n\nBut can we still go faster anyway? Honestly I doubt I can make pure javascript run this test any faster than Kevin did. But maybe.. just maybe we can be...\n\n## Faster than Javascript\n\nWhen I told Kevin that I thought I could make a CRDT implementation that's way faster than yjs, he didn't believe me. He said Yjs was already so well optimized, going a lot faster probably wasn't possible. \"Maybe a little faster if you just port it to Rust. But not a lot faster! V8 is really fast these days!!\"\n\nBut I knew something Kevin didn't know: I knew about memory fragmentation and cache coherency. Rust isn't just *faster*. Its also lower level language, and that gives us the tools we need to control allocations and memory layout.\n\n> Kevin knows this now too, and he's working on [Yrs](https://github.com/yjs/y-crdt) to see if he can claim the performance crown back.\n\nImagine an object like this in javascript:\n\n```javascript\n{ item: 'hello', isDeleted: false, id: ['seph', 10], seq, parent: ['mike', 2] }\n```\n\nThis object actually looks like this in memory:\n\n> Diagram with all the parts separated by pointers\n\nBad news: *Your computer hates this.*\n\nThis is terrible because all the data is fragmented. Its all separated by pointers.\n\nTo arrange data like this, the computer has to allocate memory one by one for each item. This is slow. Then the garbage collector needs extra data to track all of those objects, which is also slow. Later we'll need to read that data. To read it, your computer will often need to go fetch it from main memory, which - you guessed it - is slow as well.\n\nHow slow are main memory reads? [At human scale](https://gist.github.com/hellerbarde/2843375) each L1 cache read takes 0.5 seconds. And a read from main memory takes close to 2 minutes! This is the difference between a single heartbeat, and the time it takes to brush your teeth.\n\n> Interactive vis showing 100 seconds vs 0.5 seconds\n\nArranging memory like javascript does would be like writing a shopping list. But instead of \"Cheese, Milk, Bread\", your list is actually a scavenger hunt: \"Under the couch\", \"On top of the fridge\", and so on. Under the couch is a little note mentioning you need toothpaste. Needless to say, this makes doing the grocery shopping a lot of work.\n\nTo go faster, we need to squish all the data together so the computer can fetch as much as possible with each read to main memory. (So a single read of my grocery list would tell us everything we need to know). Linked lists are rarely used in the real world for exactly this reason - *memory fragmentation ruins performance*. I also want to move away from linked lists because the user *does* sometimes hop around the document, which in yjs has a linear performance cost. This doesn't happen much when text editing, but I want this code to be fast in other cases too. I don't want the program to *ever* need those long document scans.\n\nWe can't fix this in javascript. The problem with fancy data structures in javascript is that you end up needing a lot of weird objects (like fixed size arrays). All those extra objects make fragmentation worse, so as a result of all your work your programs often end up running slower anyway. This is the same limitation immutablejs has, and why its performance hasn't improved.\n\nBut we don't have to use javascript. Even when making webpages, we have WebAssembly these days. We can code this up in *anything*.\n\nTo see how fast we can *really* go, I've been quietly building [a CRDT implementation in rust](https://github.com/josephg/text-crdt-rust/). This implementation is almost identical to yjs, but it uses a B-tree instead of a linked list internally to store all of its items. Each internal node of the b-tree stores the length of all of that item's children.\n\n> B-tree diagram here\n\nThis solves both of our linear scanning problems from earlier:\n\n- When we want to find the item at position 50 000, we can just traverse across and down the tree. Trees are very tidy - we can store this whole document in just 5 levels in our tree, so that means we can find any item in about 5 reads from main memory.\n- Updating the tree is fast too. We update a leaf, then the character counts at its parent, and its parent, all the way up to the root. So again, after 5 or so steps we're done. Much better than shuffling everything in a javascript array.\n\nWe never merge edits from remote peers in this test, but I made that fast too anyway. When merging remote edits we also need to find items by their ID (eg *['seph', 100]*). My rust implementation has little index to search the b-tree by ID.\n\nI'm not using yjs's trick of caching the last location - at least not yet. It might help - I just haven't tried it yet.\n\nRust gives us total control over the memory layout, so we can pack everything in tightly. Each leaf node in my b-tree stores a block of 32 entries, all packed together in memory. Inserting like this does result in some memcpy-ing, but a little bit of memcpy is fine. Memcpy is always faster than I think it will be - its a few bytes per clock cycle. Not the epic scavenger hunt of a main memory lookup. I picked 32 by benchmarking the whole system with a few different sizes. 32 seemed to work pretty well.\n\nSpeaking of fast, how fast does it go?\n\nIf we compile this code to webassembly and drive it from javascript, we can now process the whole trace in 200 milliseconds. And if we compile it to native code and call it directly from rust, we can process this editing trace in *65 milliseconds*. That's 4500x faster than where we started with automerge. It can process 4.3 *million* operations every second.\n\n| Test                              | Time taken | RAM usage |\n|:--------------------------        | ----------:| ---------:|\n| automerge (v1.0.0-preview2)       |  291s      | 880 MB    |\n| reference-crdts (automerge / yjs) |   31s      |  28 MB    |\n| Yjs (v13.5.5)                     | 0.97s      | 3.3 MB    |\n| *JS baseline*                     | 0.61s      | 0.1 MB    |\n| **Rust (Called from JS via WASM)**| 0.20s      | ???       |\n| **Rust (native)**                 | 0.065s     | 2.3 MB    |\n\nEven through webassembly, this code is 3x faster than editing a native javascript string directly, and its doing a whole lot of extra work to support collaborative editing too!\n\n\n## Struct of arrays or Array of structs?\n\nThis implementation has another small, important change - and I'm not sure if I like it.\n\nSee, in rust I'm actually doing something like this:\n\n```javascript\ndoc = {\n  textContent: RopeyRope { 'hello' },\n\n  clients: ['seph', 'mike'],\n\n  items: RangeTree {[\n    // No string content!\n    { len: 5, id: [0, 0], seq, parent: ROOT },\n    { len: -5, id: [1, 0], seq, parent: [0, 0] }, // negative len = deleted content\n    ...\n  ]},\n}\n```\n\nNotice the document's text content doesn't live in the list of items anymore. Now its in a separate data structure. I'm using a rust library for this called [Ropey](https://crates.io/crates/ropey). Ropey implements *another* b-tree to efficiently manage just the document's text content.\n\nThis isn't universally a win. We have unfortunately arrived at the Land of Uncomfortable Engineering Tradeoffs:\n\n- Ropey can to do text-specific byte packing. So with ropey, we use less RAM.\n- But when inserting we need to update 2 data structures instead of 1. This makes everything more than twice as slow, and it makes the wasm bundle twice as big (60kb -> 120kb).\n\nSo I'm still not sure whether I like this approach.\n\nMy CRDT implementation is so fast at this point that most of the algorithm's time is spent updating the document in ropey. Ropey on its own takes 29ms to process this editing trace. What happens if I just ... turn ropey off? How fast can this puppy can really go?\n\n| Test                              | Time taken | RAM usage | CRDT structure |\n|:--------------------------        | ----------:| ---------:| ---------------|\n| automerge (v1.0.0-preview2)       |  291s      | 880 MB    | Naive tree     |\n| reference-crdts (automerge / yjs) |   31s      |  28 MB    | Array          |\n| Yjs (v13.5.5)                     | 0.97s      | 3.3 MB    | Linked list    |\n| *JS baseline*                     | 0.61s      | 0.1 MB    | *(none)*       |\n| Rust (Called from JS via WASM)    | 0.20s      | ???       | B-Tree         |\n| Rust (native)                     | 0.065s     | 2.3 MB    | B-Tree         |\n| *Ropey (rust) baseline*           | 0.029s     | 0.2 MB    | *(none)*       |\n| **Rust (native, no doc content)** | 0.023s     | 2.1 MB    | B-Tree         |\n\nBoom. This is kind of useless, but its now 14000x faster than automerge. We're processing 260 000 operations in 23ms, which is 11 million operations per second. I could saturate my home internet connection with edits I'd still have CPU to spare.\n\nThat, my friends, is how you make the computer do a lot less work.\n\nAnd oh look - those last three rows are *weird*! The full rust performance benchmark (65ms) should be the sum of the time spent in ropey (29ms) + the time spent updating my b-tree (23ms). But those numbers don't add up! The difference is probably caused by CPU cache thrashing. If I'm right, a *batch_update()* method would bring my 65ms performance all the way down to *52ms*.\n\n\n# Conclusion\n\nThat silly academic paper I read all those years ago says CRDTs and OT algorithms are slow. And everyone believed the paper, because it was Published Science. But the paper was wrong. We *can* make CRDTs fast. We can make them *crazy fast*. We can make them run so fast we can compete with the performance of native strings. The performance numbers in that paper weren't just wrong. They were \"a billionaire guessing a banana costs $1000\" kind of wrong.\n\nBut you know what? I sort of appreciate that paper now. Their mistake is ok. Its *human*. I used to feel inadequate around academics - maybe I'll never be that smart! But this whole thing made me realise something obvious: Scientists aren't gods, sent from the heavens with the gift of Truth. No, they're *people*. And in that regard, they're just like the rest of us. Great at whatever we obsess over, but kind of middling everywhere else. I can code pretty well, but I still get zuccini and cucumber mixed up. And despite being sometimes laughed at, thats ok.\n\nA decade ago Google Wave really needed a good quality list CRDT. I got super excited when the papers for CRDTs started to emerge. [LOGOOT](https://hal.inria.fr/inria-00432368/document) and [WOOT](https://hal.inria.fr/inria-00445975/document) seemed like a big deal! But that excitement died when I realised the algorithms were too slow and inefficient to be practically useful. And I made a big mistake - I assumed if the academics couldn't make them fast, nobody could.\n\nBut the truth is sometimes the best work needs a collaboration. And those collaborations sometimes work best when they involve people with different skills. Look at [Gilbert & Sullivan](https://en.wikipedia.org/wiki/Gilbert_and_Sullivan) (they wrote a bunch of famous musicals). One guy wrote the music, the other the lyrics. Or Jobs and Wozniak - one storyteller, the other the engineer. They shared a dream but that dream needed different skill sets to be achieved. Together they made something nobody could do alone.\n\nEven though I'm terrible at academic papers, I'm pretty good at making code run fast. And yet here, in my own field, I didn't even try to help. The researchers were doing their part to make P2P collaborative editing work. But I wasn't doing mine. If I did, maybe we would have had fast, workable CRDTs for text editing a decade ago. Oops! It turned out collaborative editing needed a collaboration between all of us. How ironic! Who could have guessed?!\n\nWell, it took a decade, some hard work and some great ideas from a bunch of clever folks. The binary encoding system Martin invented for Automerge is brilliant. The system of avoiding UUIDs by using incrementing (agent id, sequence) tuples is genius. I have no idea who came up with that, but I love it. And of course, Kevin's list representation + insertion approach I describe here makes everything so much faster and simpler. I bet 100 smart people must have walked right past that idea over the last decade without any of them noticing it. I doubt I would have thought of it either.\n\nAnd now, after a decade of waiting, we finally have the formula for fast, lightweight CRDT implementations. Practical decentralized realtime collaborative editing? We're coming for you next.\n\n\n# Appendix A: I want to use a CRDT for my application. What should I do?\n\nIf you're building a document based collaborative application today, you should use yjs. Yjs has excellent performance, low memory usage and great support. If you want help implementing yjs in your application, Kevin Jahns sometimes accepts money in exchange for help integrating yjs into various applications. He uses this to fund working on yjs (and adjacent work) full time. Yjs already runs fast and soon it should become even faster.\n\nMy rust code is *really* fast, but it will probably never turn into a useful general purpose CRDT library like yjs. To be able to compete with yjs, there are lots of other things it needs to do well, including binary encoding, network protocols, support for non-list structures, presence (cursor positions) and so on. Kevin has got this.\n\nIf you want database semantics instead of document semantics, as far as I know nobody has done this well on top of CRDTs yet. You can use [ShareDB](https://github.com/share/sharedb/), which uses OT. I wrote sharedb years ago, and has been continually improved since then.\n\nLooking forward, I'm excited for [Redwood](https://github.com/redwood/redwood) - which supports P2P editing and has planned full CRDT support.\n\n\n# Appending B: Are these benchmarks for real?\n\nYes. But there are a few slippery sleights of hand going on here.\n\nIf you want to play with any of the benchmarks I ran yourself, everything is a bit of a hodge podge mess.\n\nThe benchmark code for the JS string baseline, yjs, automerge and reference-crdts tests is all in [this github gist](https://gist.github.com/josephg/13efc1444660c07870fcbd0b3e917638). Its a mess; but messy code is better than missing code.\n\nFor my rust implementation results come from benchmarking [josephg/text-crdt-rust, at this version](https://github.com/josephg/text-crdt-rust/tree/ba20b6386c0472958f33024ce0b806e75470e1ca). Benchmark by running `cargo criterion yjs` or `cargo criterion ropey`, if you want to isolate the ropey baseline. The inline rope structure updates can be enabled or disabled by editing the constant at the top of src/universal/doc.rs. You can also add `--features memusage` to print out memory allocator statistics while benchmarking. (Note you can't accurately measure memory usage in the unit testing environment. I've been running the benchmarks with that flag, then killing the benchmark run as soon as numbers start coming out).\n\nYou'll also need `automerge-paper.json.gz` from [josephg/crdt-benchmarks](https://github.com/josephg/crdt-benchmarks) in order to run most of these tests. The reference-crdts benchmark depends on [crdts.ts from josephg/reference-crdts, at this version](https://github.com/josephg/reference-crdts/tree/fed747255df9d457e11f36575de555b39f07e909)\n\nI haven't uploaded my rust crdt wasm wrapper anywhere.\n\n### Hang on - are these CRDTs actually the same?\n\nThrough this post I'm comparing the performance of implementations of RGA (automerge) and YATA (yjs + my rust implementation) interchangeably.\n\nDoing this rests on the assumption that the semantics for YATA and RGA are basically the same, and that you can swap between CRDT semantics without changing your implementation, or your implementation performance. This is a novel idea that I think nobody has looked at before.\n\nI feel confident in this claim because I demonstrated this in my [reference CRDT implementation](https://github.com/josephg/reference-crdts), which has identical performance (and a almost-identical codepath) when using yjs or automerge's semantics. There might be some performance differences with conflict-heavy editing traces - but that's extremely rare in practice.\n\nI'm also confident you could modify yjs to implement RGA semantics if you wanted to, without changing yjs's performance. I know how you'd do it, too:\n\n- Change Yjs's *integrate* method (or make an alternative) which used slightly different logic for concurrent edits\n- Store *seq* instead of *originRight* in each *Item*\n- Store *maxSeq* in the document, and keep it up to date and\n- Change yjs's binary encoding format.\n\nI talked to Kevin about this, and he doesn't see any point in adding RGA support into his library. Its not something anybody actually asks for.\n\nFor my rust code, I probably will at some point make my CRDT implementation accept a type parameter which switches between yjs and automerge semantics.\n\n### This is the wrong measure of performance\n\nYes, I know and I agree. This post only measures the time taken to replay a local editing trace, and the resulting RAM usage. But arguably accepting incoming changes from the user only needs to happen fast *enough*. Fingers simply don't type very fast - so once a CRDT can handle any local user edit in under about 1ms, going faster probably doesn't matter much.\n\nIt is really fun though.\n\nThe *actually important* metrics are:\n\n- How many bytes a document takes on disk or over the network\n- How much time the document takes to save and load\n- The time taken to update a document at rest (more below)\n\nThe editing trace I'm using here also only has a single user making edits. There could be pathological performance cases lurking in the shadows when users make concurrent edits.\n\nI did it this way because I haven't implemented a binary format in my reference-crdts implementation or my rust code. If I did, I'd probably copy yjs & automerge's binary formats because they're so compact. So I expect the resulting size would be identical between all of these implementations. And I suspect performance for loading and saving would be pretty similar to the time taken to process editing traces. But I've been wrong before.\n\nThere is one difference which might matter - yjs and automerge treat information about deleted items slightly differently. (Yjs run-length-encodes them and packs that information into the version structure. Automerge puts deletes into the operation log.) This probably has some implications in terms of implementation, because for automerge you need to store *when* each delete happened. Not just *if* the item has been deleted.\n\n---\n\nThe other performance measure we need to take a lot more seriously is how we update a document at rest. Most applications aren't collaborative text editors. Usually applications have databases full of tiny objects. Each of those objects is very rarely written to - so the model of \"load an object, keep it in ram while its open and then eventually save it\" doesn't apply.\n\nIf you want to update a single object in a database using yjs or automerge today you need to:\n\n1. Load the whole document into RAM\n2. Make your change\n3. Save the whole document back to disk again\n\nThis is going to be awfully slow. There are better approaches than this - but as far as I know, nobody is working on this at all. We could use your help!\n\n> Edit: Kevin says you can adapt Yjs's providers to implement this in a reasonable way. I'd love to see that in action.\n\n---\n\nI'm also not looking at pruning here. CRDTs like this grow over time (since we keep tombstones from all deleted items). This can be addressed (eg Yjs's GC algorithm, or Antimatter). Or you could just mostly ignore it like git does - nobody seems to care that their git repos grow without bound, forever.\n\nBut that's orthogonal to everything I've listed above.\n\n\n### You have too many variables changing\n\nYeah, well spotted. Each of these tests changes multiple variables. Moving from automerge to my reference-crdts resulted in changes to:\n\n- The core data structure (tree to list)\n- Removed immutablejs\n- Removed automerge's frontend / backend protocol. And all those Uint8Arrays that pop up throughout automerge for whatever reason are gone too, obviously.\n- The javascript style is totally different. (FP javascript -> imperative)\n\nWe got 10x performance from this change. But I can only guess how to assign that 10x speedup amongst all of the differences.\n\nThe changes from reference-crdts to yjs, and from yjs to my rust code are similarly monolithic. How much of the speed difference between my rust code and yjs has nothing to do with memory layout, and everything to do with the rust compiler?\n\nI don't know!\n\nSo, yes. This is a reasonable criticism of this approach. If this bothers you, I'd *love* for someone to pull apart each of these performance differences between implementations I show here and tease apart a more detailed guide. I'd read the heck out of that. I love benchmarking stories. That's normal, right?\n\n\n# Appendix C: I don't get it - why is automerge's javascript so slow?\n\nIts really just not trying to be fast. Look at this code:\n\n```javascript\nfunction lamportCompare(op1, op2) {\n  return opIdCompare(op1.get('opId'), op2.get('opId'))\n}\n\nfunction insertionsAfter(opSet, objectId, parentId, childId) {\n  let childKey = null\n  if (childId) childKey = Map({opId: childId})\n\n  return opSet\n    .getIn(['byObject', objectId, '_following', parentId], List())\n    .filter(op => op.get('insert') && (!childKey || lamportCompare(op, childKey) < 0))\n    .sort(lamportCompare)\n    .reverse() // descending order\n    .map(op => op.get('opId'))\n}\n```\n\nThis is called on each insert, to figure out how the children of an item should be sorted. I don't know how hot it is, but there are *so many things* slow about this:\n\n- I can spot 7 allocations in this function. (Though the 2 closures should be hoisted). (Can you find them all?)\n- The items are already sorted reverse-lamportCompare before this method is called. Sorting an anti-sorted list is the slowest way to sort anything. Rather than sorting, then reverse()'ing, this code should just invert the arguments in `lamportCompare` (or negate the return value).\n- The goal is to insert a new item into an already sorted list. You can do that much faster with a for loop.\n- This code wraps childId into an immutablejs Map, just so the argument matches `lamportCompare` - which then unwraps it again. Stop - I'm dying!\n\nBut in practice this code is going to be replaced by WASM calls through to [automerge-rs](https://github.com/automerge/automerge-rs). Maybe it already has been replaced with automerge-rs by the time you're reading this! So it doesn't matter. Try not to think about it. Definitely don't submit any PRs to fix all the low hanging fruit. *twitch.*\n\n\n"